{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b00dae2-c271-4a69-a579-742e084a9058",
   "metadata": {},
   "source": [
    "# Lab | Langchain Evaluation\n",
    "\n",
    "## Intro\n",
    "\n",
    "Pick different sets of data and re-run this notebook. The point is for you to understand all steps involve and the many different ways one can and should evaluate LLM applications.\n",
    "\n",
    "What did you learn? - Let's discuss that in class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c417b",
   "metadata": {},
   "source": [
    "Goal: This whole section is about learning how to test your AI. Not just build it ‚Äî but test if it answers right, smart, and on point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "## LangChain: Evaluation\n",
    "\n",
    "### Outline:\n",
    "\n",
    "Example Generation üß†\n",
    "\n",
    "Manual Evaluation (you be the judge) ‚úã\n",
    "\n",
    "LLM-assisted Evaluation (AI be the judge) ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e29d2c-ba67-4cba-8ded-375fe040b9ba",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28008949",
   "metadata": {},
   "source": [
    "#### Create our QandA application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import CSVLoader, TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1106d",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = 'data/OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550eb642-c223-4d78-8f92-0f265ef78b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " !pip install --upgrade --force-reinstall sentence-transformers\n",
    "\n",
    "## makin' sure the sentence embedding tool is fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdde79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35675b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c218f",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs = {'device': 'cpu'})\n",
    ").from_loaders([loader])\n",
    "\n",
    "\n",
    "## turnin' all your text into brainy math (embeddings)\n",
    "#  so AI can find what it needs real fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2006054",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=index.vectorstore.as_retriever(), \n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "#  settin' up our smart Q&A bot \n",
    "#  strict tone, no randomness, answers only from the catalog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ebd73",
   "metadata": {},
   "source": [
    "#### Coming up with test datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04a0f9",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[10]\n",
    "\n",
    "# peepin' at row 10 in your product catalog ‚Äî makin' sure it loaded right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a88c2",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[11]\n",
    "\n",
    "# quick look at row 11 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d548aef",
   "metadata": {},
   "source": [
    "#### Hard-coded examples\n",
    "\n",
    "This means writing your own questions + expected answers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106fbd91-f7bc-4d6b-b090-54b6a485ce39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# bringin‚Äô in the tool to make your prompt look fresh and structured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d5af6-36db-4421-b635-46384e677847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
    "        have side pockets?\",\n",
    "        \"answer\": \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What collection is the Ultra-Lofty \\\n",
    "        850 Stretch Down Hooded Jacket from?\",\n",
    "        \"answer\": \"The DownTek collection\"\n",
    "    }\n",
    "]\n",
    "# feeding the AI examples so it knows how to answer\n",
    "\n",
    "\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Examples:\\n\"\n",
    "             \"1. Query: Do the Cozy Comfort Pullover Set have side pockets?\\n\"\n",
    "             \"   Answer: Yes\\n\"\n",
    "             \"2. Query: What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\\n\"\n",
    "             \"   Answer: The DownTek collection\\n\"\n",
    "             \"Query: {query}\\n\"\n",
    "             \"Answer:\"\n",
    ")\n",
    "# prompt layout ‚Äî tellin‚Äô the AI how to spit out answers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the output model\n",
    "class Answer(BaseModel):\n",
    "    answer: str = Field(description=\"The answer to the query\")\n",
    "# setting up the answer format ‚Äî just keep it clean.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the output parser\n",
    "class AnswerOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> Answer:\n",
    "        # Split the response to get the answer\n",
    "        answer = text.strip().split(\"Answer:\")[-1].strip()\n",
    "        return Answer(answer=answer)\n",
    "\n",
    "# Initialize the LLM\n",
    "# llm = OpenAI()\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    output_parser=AnswerOutputParser()\n",
    ")\n",
    "# bundling all this into one smooth Q&A chain\n",
    "\n",
    "\n",
    "\n",
    "# Example query\n",
    "query = \"Is the Cozy Comfort Pullover Set available in different colors?\"\n",
    "\n",
    "# Run the chain\n",
    "result = llm_chain.run({\"query\": query})\n",
    "\n",
    "# est question and seein' what the bot says üëÄ\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce3e4f",
   "metadata": {},
   "source": [
    "#### LLM-Generated examples\n",
    "\n",
    "\n",
    "Let the AI generate questions for you based on the docs. This is fast but needs checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f8376",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "# importin‚Äô the tool to let the AI write test questions for you üß†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e87816",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI())\n",
    "# settin‚Äô up the test question generator with ChatGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb34772-368f-4b5e-b4bd-da9b637cc7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "# re-bootin' the LLM chain ‚Äî keepin' it tight and ready for more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abae09",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in data[:5]]\n",
    ")\n",
    "\n",
    "# lettin‚Äô the AI cook up Q&A pairs based on the first 5 docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab28b5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_examples[0]\n",
    "\n",
    "# peekin' at the first AI-made test question üëÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe4228",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[0]\n",
    "\n",
    "# checkin' the first product doc ‚Äî makin‚Äô sure it matches the Q&A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693fe86-feeb-4d73-b400-e66e79315274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_flattened = [data['qa_pairs'] for data in new_examples]\n",
    "d_flattened\n",
    "\n",
    "\n",
    "# grabbin‚Äô the Q&A pairs from each generated example \n",
    "# flattening the goods \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf25f2f",
   "metadata": {},
   "source": [
    "#### Combine examples\n",
    "\n",
    "Mix both: your questions + AI-generated ones. Now you‚Äôve got a whole test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2a3fc",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# examples += new_example\n",
    "examples += d_flattened\n",
    "\n",
    "# stackin' the new AI-made questions on top of the old ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184b9d7-22ab-43a5-9ba5-b27fef024874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples[0]\n",
    "\n",
    "# lookin' at the first test example ‚Äî what we askin' and expectin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf5cf5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(examples[0][\"query\"])\n",
    "\n",
    "# droppin‚Äô the question into the Q&A bot ‚Äî see what it spits back\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3cb08",
   "metadata": {},
   "source": [
    "### Manual Evaluation - Fun part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf622e",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "# turnin‚Äô on detective mode ‚Äî \n",
    "# logs everything so you see what‚Äôs poppin' under the hood \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a142638",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.invoke(examples[0][\"query\"])\n",
    "\n",
    "# rerunnin' the question with debug on \n",
    "# seein' all the behind-the-scenes sauce \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6bef0",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn off the debug mode\n",
    "langchain.debug = False\n",
    "\n",
    "# chillin' the logs ‚Äî turnin‚Äô the noise off \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bdbdce",
   "metadata": {},
   "source": [
    "### LLM assisted evaluation\n",
    "\n",
    "this is the part where you let the AI judge the AI based on how relevant, faithful, or on-topic the answers are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54769b0-3daf-4cac-b259-89a10dd9b5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples += d_flattened\n",
    "\n",
    "# addin' all the new AI-made Q&A test data to the original example stack \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15038c0a",
   "metadata": {},
   "source": [
    "# ‚ö°the full list of test examples ‚Äî  AI's pop quiz sheet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea95385-1b4c-440a-9fea-8500b4cc2154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dca05a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = qa.batch(examples)\n",
    "\n",
    "# feedin‚Äô all examples into your smart Q&A bot ‚Äî gettin‚Äô answers in one go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e8b4e-4468-4048-8544-c9936704ea93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae62ebf",
   "metadata": {},
   "source": [
    "##  bringin‚Äô in the LangChain tool to judge if our bot's answers slap or flop ‚öñÔ∏è\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6012a3e0",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b1c0b",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "#  settin' up a serious AI judge (no randomness) to grade the answers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46ae55",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "graded_outputs = eval_chain.evaluate(examples, predictions)\n",
    "\n",
    "# lettin‚Äô the judge AI grade your bot‚Äôs answers based on truth & quality üíØ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d95cb",
   "metadata": {},
   "source": [
    "### peek at the report card ‚Äî this shows how your bot did on each üò¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42eb35-c2d7-4581-8004-d315ade63eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437cfbe",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    # print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()\n",
    "\n",
    "\n",
    "    # loopin‚Äô through each test Q \n",
    "    # showin' what the AI was asked, what it said\n",
    "    #  and what the real answer was \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d127a-a9e3-465d-a8ae-0e2c4b4a2659",
   "metadata": {},
   "source": [
    " ## Example 2 ‚Äî Using ragas üí•\n",
    "One can also easily evaluate your QA chains with the metrics offered in ragas\n",
    "\n",
    "\n",
    "\n",
    " ## Here Text doc about NYC and asking it real-world questions using that same LangChain power üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef0493-34ff-4801-b405-69c76ce86c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "loader = TextLoader(\"data/nyc_text.txt\")\n",
    "index = VectorstoreIndexCreator(embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",\n",
    "                                                                 model_kwargs = {'device': 'mps'})).from_loaders([loader])\n",
    "# # 'mps' means it's runnin' on Apple Silicon MacBook GPU power\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68310fa0",
   "metadata": {},
   "source": [
    "# üîπ Set up the LLM Q&A chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeeafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature= 0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a963db7",
   "metadata": {},
   "source": [
    "# üîπ Ask your question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0449cae-de25-4ef6-ae64-78ccf5e06a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing it out\n",
    "\n",
    "question = \"How did New York City get its name?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "result[\"result\"]\n",
    "# throwin‚Äô the question at the chain \n",
    "# Get the final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008807a",
   "metadata": {},
   "source": [
    "## View the whole result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e846b3d-f79f-46eb-8075-c816268c0500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d9da8-a593-4fc6-9d4b-fea2af6bdfd0",
   "metadata": {},
   "source": [
    "Now in order to evaluate the qa system we generated a few relevant questions. We've generated a few question for you but feel free to add any you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2cade-0005-41c1-b775-c6a7175bcf3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"What is the population of New York City as of 2020?\",\n",
    "    \"Which borough of New York City has the highest population?\",\n",
    "    \"What is the economic significance of New York City?\",\n",
    "    \"How did New York City get its name?\",\n",
    "    \"What is the significance of the Statue of Liberty in New York City?\",\n",
    "]\n",
    "\n",
    "eval_answers = [\n",
    "    \"8,804,190\",\n",
    "    \"Brooklyn\",\n",
    "    \"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\",\n",
    "    \"New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.\",\n",
    "    \"The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.\",\n",
    "]\n",
    "\n",
    "# these are your gold-standard answers ‚Äî like the answer key for a quiz üíØ\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\"query\": q, \"ground_truths\": [eval_answers[i]]}\n",
    "    for i, q in enumerate(eval_questions)\n",
    "]\n",
    "\n",
    "\n",
    "# bundling Qs and their matching correct As into one package üì¶\n",
    "# this format is ready to be passed into an evaluator \n",
    "# (like Ragas or LangChain QA Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9358e-f8bc-4992-aea3-c83160ff0ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21efe8-7c30-449a-9b8e-5b79778e305b",
   "metadata": {},
   "source": [
    "#### Introducing RagasEvaluatorChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c2214-a6eb-4d4f-9403-7e1574b97a36",
   "metadata": {},
   "source": [
    "`RagasEvaluatorChain` creates a wrapper around the metrics ragas provides (documented [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md)), making it easier to run these evaluation with langchain and langsmith.\n",
    "\n",
    "The evaluator chain has the following APIs\n",
    "\n",
    "- `__call__()`: call the `RagasEvaluatorChain` directly on the result of a QA chain.\n",
    "- `evaluate()`: evaluate on a list of examples (with the input queries) and predictions (outputs from the QA chain). \n",
    "- `evaluate_run()`: method implemented that is called by langsmith evaluators to evaluate langsmith datasets.\n",
    "\n",
    "lets see each of them in action to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d76ad",
   "metadata": {},
   "source": [
    "# üí¨ Ask a question + get an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c8aae-fe5f-4274-b638-9209151b9491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = qa_chain.invoke({\"query\": eval_questions[1]})\n",
    "result[\"result\"]\n",
    "\n",
    "\n",
    "# askin' one of your real test questions \n",
    "# get the bot's answer back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae31c80-42c9-4b1f-95b3-c05ceadb103f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map your keys to Ragas format\n",
    "\n",
    "key_mapping = {\n",
    "    \"query\": \"question\",\n",
    "    \"result\": \"answer\",\n",
    "    \"source_documents\": \"contexts\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#  Convert  result\n",
    "result_updated = {}\n",
    "for old_key, new_key in key_mapping.items():\n",
    "    if old_key in result:\n",
    "        result_updated[new_key] = result[old_key]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2a998",
   "metadata": {},
   "source": [
    "# This is your cleaned-up result ‚Äî ready to be judged by the RagasEvaluatorChain üßº\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4dd9f-16d7-43d4-ac8e-6c5aa5e3f7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5cc30-d25e-41bd-8695-9246b73938bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir recordclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d55d9-437d-40c4-bb5f-87c7b65fa567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ragas==0.1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b14c07-bf6c-4e86-ad1a-2a6c4d1a509d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain \n",
    "# from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "# these are the 4 big judge styles Ragas uses to score your bot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0aeb8",
   "metadata": {},
   "source": [
    "# üèÜ Set up the Judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bf223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation chains\n",
    "faithfulness_chain   = EvaluatorChain(metric=faithfulness)\n",
    "answer_rel_chain     = EvaluatorChain(metric=answer_relevancy)\n",
    "context_rel_chain    = EvaluatorChain(metric=context_relevancy)\n",
    "context_recall_chain = EvaluatorChain(metric=context_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a8f30",
   "metadata": {},
   "source": [
    "faithfulness_chain ‚Äì checks if your AI is makin' stuff up or stickin' to the source  (truth checker)\n",
    "\n",
    "answer_rel_chain ‚Äì checks if the answer actually hits what the question was askin'\n",
    "\n",
    "context_rel_chain ‚Äì checks if the retrieved docs even match the question topic \n",
    "\n",
    "context_recall_chain ‚Äì checks if the real answer was even in the docs pulled "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8b636-d738-41bd-ac68-21cce2c4b720",
   "metadata": {},
   "source": [
    "1. `__call__()`\n",
    "\n",
    "Directly run the evaluation chain with the results from the QA chain. Do note that metrics like context_relevancy and faithfulness require the `source_documents` to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7815671c-1fc8-46ba-8356-4a0bd5558530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the result that we are going to validate.\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdd07c-956c-458f-8844-e056f29e3c83",
   "metadata": {},
   "source": [
    "**Faithfulness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f5f0f-a237-4584-becb-a35607caf26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = faithfulness_chain(result_updated)\n",
    "eval_result[\"faithfulness_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd78a9-d7c5-42a0-8705-4c544ec6408e",
   "metadata": {},
   "source": [
    "High faithfulness_score means that there are exact consistency between the source documents and the answer.\n",
    "\n",
    "You can check lower faithfulness scores by changing the result (answer from LLM) or source_documents to something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f42b97-a84f-4015-9da5-fa3b0b703c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_result = result.copy()\n",
    "fake_result[\"result\"] = \"we are the champions\"\n",
    "eval_result = faithfulness_chain(fake_result)\n",
    "eval_result[\"faithfulness_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02348380-159a-4578-a95e-493cdcfcebe7",
   "metadata": {},
   "source": [
    "**Context Relevancy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2fb9e-19d1-4cf6-9773-897546c2d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = context_recall_chain(result)\n",
    "eval_result[\"context_recall_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118cba80-c2a3-408e-9d8d-857521cbe723",
   "metadata": {},
   "source": [
    "High context_recall_score means that the ground truth is present in the source documents.\n",
    "\n",
    "You can check lower context recall scores by changing the source_documents to something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6610c5f-5f8e-406c-885c-093012a5dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "fake_result = result.copy()\n",
    "fake_result[\"source_documents\"] = [Document(page_content=\"I love christmas\")]\n",
    "eval_result = context_recall_chain(fake_result)\n",
    "eval_result[\"context_recall_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc06403-c514-47fd-ac82-d95ece8d2f06",
   "metadata": {},
   "source": [
    "2. `evaluate()`\n",
    "\n",
    "\n",
    "‚úÖLike grading a whole exam instead of one question.‚úÖ\n",
    "\n",
    "Evaluate a list of inputs/queries and the outputs/predictions from the QA chain.\n",
    "\n",
    "# ‚úÖ 1.  Evaluate Faithfulness (Truth Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b23708-b94d-4649-acd9-0e268f72f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the queries as a batch for efficiency\n",
    "predictions = qa_chain.batch(examples)\n",
    "\n",
    "# evaluate\n",
    "print(\"evaluating...\")\n",
    "r = faithfulness_chain.evaluate(examples, predictions)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf873fbc",
   "metadata": {},
   "source": [
    "# üìä 2. Evaluate Context Recall (Did it use the source?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661f6b9-04c7-40b7-874b-25f53cfab9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate context recall\n",
    "print(\"evaluating...\")\n",
    "r = context_recall_chain.evaluate(examples, predictions)\n",
    "r\n",
    "\n",
    "# This tells you whether the bot even retrieved \n",
    "# the right context to answer the question\n",
    "#  *super important in real-world apps like legal, health, etc*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
